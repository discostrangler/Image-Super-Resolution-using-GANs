# Photo-Realistic Image Super-Resolution using SRGAN

## üß† Overview

This repository presents an implementation of **SRGAN (Super-Resolution Generative Adversarial Network)** for generating **photo-realistic high-resolution images** from low-resolution inputs. Based on the seminal paper *‚ÄúPhoto-Realistic Single Image Super-Resolution Using a Generative Adversarial Network‚Äù* by Ledig et al. (CVPR 2017), this work focuses on improving perceptual quality through **adversarial training** and **perceptual loss functions** instead of traditional pixel-wise optimization.

The model is capable of producing sharper textures and fine-grained details that are visually closer to the ground-truth images compared to MSE-based methods. Our implementation aims to replicate the SRGAN architecture while introducing training enhancements and modular code organization for easier experimentation.

---

## üìö Abstract

While deep CNN-based methods achieve high PSNR values for image super-resolution, they often fail to generate visually realistic textures. Minimizing **Mean Squared Error (MSE)** leads to overly smooth reconstructions lacking high-frequency details. The **SRGAN framework** addresses this limitation by introducing a **perceptual loss** that combines an **adversarial loss** (encouraging photo-realism) and a **content loss** computed using VGG feature maps, which capture perceptually meaningful representations rather than raw pixel similarity.

The generator network employs a deep residual network (ResNet) architecture with skip connections, while the discriminator is trained to distinguish super-resolved images from real high-resolution ones. Together, they form an adversarial system that pushes the generator to produce outputs lying closer to the manifold of natural images.

---

## ‚öôÔ∏è Methodology

### 1. Generator Network

The **generator** is a deep ResNet-based model consisting of 16 residual blocks with **3√ó3 convolutions**, **batch normalization**, and **Parametric ReLU (PReLU)** activations. The upscaling process is performed using **sub-pixel convolutional layers** to efficiently increase resolution without interpolation.

### 2. Discriminator Network

The **discriminator** is a convolutional network designed to classify images as real or generated. It uses strided convolutions, **Leaky ReLU** activations, and progressively increases feature channels up to 512, following VGG-style design principles. The output layer uses a sigmoid activation for binary classification.

### 3. Perceptual Loss Function

The total **perceptual loss** combines content and adversarial components:

* **Content Loss (VGG-based):** Computed as the Euclidean distance between the feature maps of generated and ground truth images from specific layers (e.g., *VGG19 conv5_4*). This captures high-level perceptual similarity.
* **Adversarial Loss:** Encourages the generator to produce outputs that the discriminator classifies as real, improving realism and texture richness.

### 4. Training Strategy

* The generator is first pre-trained using MSE loss for stability.
* Adversarial training alternates between optimizing generator and discriminator.
* Optimization uses the **Adam optimizer** with learning rate scheduling.
* Batch normalization is disabled during inference for deterministic outputs.

---

## üß© Dataset

The model is trained on images from the **ImageNet** dataset for diversity and evaluated using standard super-resolution benchmarks:

* **Set5**
* **Set14**
* **BSD100**

Low-resolution images are generated by bicubic downsampling (scale factor 4√ó). Evaluation metrics include **PSNR**, **SSIM**, and **Mean Opinion Score (MOS)** for perceptual quality assessment.

---

## üìà Results

| Dataset | Model          | PSNR (‚Üë) | SSIM (‚Üë) | MOS (‚Üë)  |
| ------- | -------------- | -------- | -------- | -------- |
| Set5    | SRResNet (MSE) | 32.05    | 0.9019   | 3.37     |
| Set5    | SRGAN (VGG54)  | 29.40    | 0.8472   | **3.58** |
| Set14   | SRResNet (MSE) | 28.49    | 0.8184   | 2.98     |
| Set14   | SRGAN (VGG54)  | 26.02    | 0.7397   | **3.72** |

The SRGAN model achieves significantly higher **perceptual realism** despite slightly lower PSNR. In MOS testing, its outputs were consistently rated closer to ground-truth images by human evaluators.

---

## üß™ Key Contributions

* Implemented a **ResNet-based Generator** with skip connections and sub-pixel upscaling.
* Designed a **Discriminator Network** for adversarial training.
* Adopted a **VGG feature-based content loss** for perceptual similarity.
* Achieved **photo-realistic textures** and visually convincing results on public benchmarks.

---

## üìä Evaluation Metrics

* **PSNR (Peak Signal-to-Noise Ratio)** ‚Äì measures reconstruction fidelity.
* **SSIM (Structural Similarity Index)** ‚Äì measures structural preservation.
* **MOS (Mean Opinion Score)** ‚Äì human-perceived visual quality rating.

---

## üß† Architecture Summary

```
Low-Resolution Image  
   ‚Üì  
Generator Network (Residual Blocks + Subpixel Layers)  
   ‚Üì  
Discriminator Network (Conv Layers + Sigmoid Output)  
   ‚Üì  
Adversarial Training Loop (Generator ‚Üî Discriminator)  
   ‚Üì  
High-Resolution Output
```
